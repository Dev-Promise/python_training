import requests
from bs4 import BeautifulSoup

# Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
url = "https://news.yahoo.co.jp/"
response = requests.get(url)

# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
if response.status_code != 200:
    print("ãƒšãƒ¼ã‚¸å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
    exit()

# HTMLã‚’è§£æ
soup = BeautifulSoup(response.text, "html.parser")

# aã‚¿ã‚°ã‚’ã™ã¹ã¦å–å¾—
all_links = soup.find_all("a")

# ãƒ‹ãƒ¥ãƒ¼ã‚¹å€™è£œã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
results = []

print("ğŸ“° Yahooãƒ‹ãƒ¥ãƒ¼ã‚¹ ãƒˆãƒƒãƒ—è¨˜äº‹ï¼ˆæ¨å®šï¼‰:")
count = 0
for link in all_links:
    text = link.get_text(strip=True)
    href = link.get("href")
    # æ¡ä»¶ã«åˆã†ãƒªãƒ³ã‚¯ã ã‘æŠ½å‡º
    if (
        text
        and len(text) >= 15  # ã‚¿ã‚¤ãƒˆãƒ«ã£ã½ã„æ–‡å­—æ•°
        and href
        and "news.yahoo.co.jp" in href
    ):
        print(f"- {text}({href})")
        results.append(f"{text} ({href})")
        count += 1
    if count >= 5:
        break

# âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
with open("yahoo_news.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(results))

print("\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ« 'yahoo_news.txt' ã«ä¿å­˜ã—ã¾ã—ãŸï¼")
